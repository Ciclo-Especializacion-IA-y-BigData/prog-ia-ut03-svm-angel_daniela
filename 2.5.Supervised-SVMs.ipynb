{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sTwnMzPLz3bP"
   },
   "source": [
    "# Support Vector Machines\n",
    "## Cristina Gómez Alonso y Carlos Tessier\n",
    "\n",
    "En este notebook tomaremos como referencia el material del brillante Aurélien Géron, complementado con apuntes de Akranz y recursos actualizados hasta 2025.\n",
    "Los SVM constituyen una **familia de algoritmos supervisados** que pueden trabajar en **clasificación**, **regresión** (*Support Vector Regression*) e incluso **detección de valores atípicos** (*One-Class SVM*). A pesar de ser un método relativamente “clásico” dentro del Machine Learning, siguen siendo altamente competitivos en datasets pequeños o medianos y con un número elevado de variables.\n",
    "\n",
    "Los SVM destacan por:\n",
    "\n",
    "* Ser inicialmente **algoritmos de clasificación binaria** (aunque existen extensiones multiclase, como *one-vs-rest* o *one-vs-one*).\n",
    "* Construir un **hiperplano de separación** en un espacio N-dimensional que divide los datos en dos clases.\n",
    "* Buscar siempre la **separación con mayor margen posible**, lo que conduce a modelos generalmente robustos.\n",
    "* Funcionar especialmente bien en conjuntos con **pocas muestras pero alta dimensionalidad**.\n",
    "* Poder representar **fronteras no lineales** mediante **kernels** (polinomial, RBF/Gaussiano, sigmoide, etc.).\n",
    "* Ser sensibles a la necesidad de **escalado de características**.\n",
    "\n",
    "## 1. Clasificación Lineal con SVM\n",
    "\n",
    "La idea fundamental de un SVM lineal se entiende observando el siguiente esquema:\n",
    "\n",
    "<div style=\"text-align:center;\"><img  src=\"./img/SVM_example.png\" /></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "vlkJ6bSOz3bV",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Si dos clases son **linealmente separables**, cualquier recta (o hiperplano) que las separe *funciona* en entrenamiento. Sin embargo, no todas tienen la misma capacidad de generalizar.\n",
    "En el gráfico de la izquierda se muestran varios límites de decisión posibles. Algunos separan correctamente las clases, pero están tan ajustados a los puntos que probablemente rendirán mal en datos nuevos.\n",
    "\n",
    "El modelo de la derecha maximiza la distancia entre el hiperplano y los puntos más cercanos de cada clase. Estas instancias se denominan **vectores de soporte**, porque determinan por completo la frontera. Añadir más puntos lejos de esa “calle” no cambiará el hiperplano.\n",
    "\n",
    "La búsqueda del hiperplano que genere la “calle más ancha posible” recibe el nombre de:\n",
    "\n",
    "### **Clasificación de margen máximo (Large Margin Classification)**\n",
    "\n",
    "Este enfoque tiende a producir clasificadores robustos porque evita decisiones demasiado dependientes de unos pocos puntos ruidosos (salvo que estos queden justo en el margen, algo que veremos a continuación).\n",
    "\n",
    "Es importante recordar que:\n",
    "\n",
    "* Un hiperplano en N dimensiones es un objeto de dimensión N−1.\n",
    "* La frontera depende únicamente de los vectores de soporte.\n",
    "* Si las características están en **escalas distintas**, el margen puede distorsionarse gravemente. Por eso en SVM es **obligatorio estandarizar** o normalizar.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WejzQQBuz3bW"
   },
   "source": [
    "### 1.1. Clasificación de margen suave (*Soft Margin Classification*)\n",
    "\n",
    "El caso ideal sería encontrar un hiperplano que deje **todas las muestras bien clasificadas y fuera del margen**. Esto se denomina **Hard Margin SVM**, pero tiene dos problemas serios:\n",
    "\n",
    "1. Solo funciona si los datos son perfectamente separables.\n",
    "2. Es extremadamente sensible al ruido y a los *outliers*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "5bJgYmubz3bX",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div style=\"text-align:center;\"><img style=\"width:66%;\" src=\"./img/Hard_Margin_Classifier.png\" /></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "RM8rYUO5z3bY",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Para superar estas limitaciones surge la **Clasificación de margen suave**, en la que permitimos infracciones del margen pero **las penalizamos** mediante el hiperparámetro `C`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "FpCkNKEiz3bY",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div style=\"text-align:center;\"><img style=\"width:66%;\" src=\"./img/Soft_to_Hard_Street.png\" /></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "DxS83IhQz3bZ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Interpretación práctica de `C`:\n",
    "\n",
    "* **C grande ⇒ margen rígido**. Se penaliza mucho cada error, riesgo de *overfitting*.\n",
    "* **C pequeño ⇒ margen flexible**. Se aceptan errores para ganar generalización.\n",
    "\n",
    "En 2025, además, conviene recordar que:\n",
    "\n",
    "* Para problemas **lineales** y datasets medianos, `LinearSVC` (basado en optimización lineal) es mucho más rápido que `SVC(kernel='linear')`.\n",
    "* `LinearSVC` no usa *hinge loss* estándar por defecto, sino una variante optimizada.\n",
    "* Si queremos probabilidades, ni `LinearSVC` ni `SVC` lineal las producen directamente; es necesario usar **calibradores como Platt scaling o CalibratedClassifierCV**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Ejemplo práctico con Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1510,
     "status": "ok",
     "timestamp": 1678287891520,
     "user": {
      "displayName": "Ernesto Frías Nores",
      "userId": "04050836756016798788"
     },
     "user_tz": -60
    },
    "id": "j-7V7J30z3ba"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Carga del dataset Iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1678287895035,
     "user": {
      "displayName": "Ernesto Frías Nores",
      "userId": "04050836756016798788"
     },
     "user_tz": -60
    },
    "id": "b3-9Ek2ez3bc"
   },
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1678287891525,
     "user": {
      "displayName": "Ernesto Frías Nores",
      "userId": "04050836756016798788"
     },
     "user_tz": -60
    },
    "id": "PUu_cWGMz3bd",
    "outputId": "74fc40c1-5e73-4598-e112-1932d9a5c37c"
   },
   "outputs": [],
   "source": [
    "iris['data'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El dataset Iris tiene:\n",
    "\n",
    "* 150 muestras\n",
    "* 4 características:\n",
    "\n",
    "  * Sepal Length\n",
    "  * Sepal Width\n",
    "  * Petal Length\n",
    "  * Petal Width\n",
    "* 3 clases: *Setosa*, *Versicolor* y *Virginica*.\n",
    "\n",
    "Para simplificar el problema y poder visualizarlo luego, nos quedamos solo con **Petal Length** (longitud del pétalo) y **Petal Width** (anchura del pétalo), y lo convertimos en un problema **binario**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 380,
     "status": "ok",
     "timestamp": 1678287899439,
     "user": {
      "displayName": "Ernesto Frías Nores",
      "userId": "04050836756016798788"
     },
     "user_tz": -60
    },
    "id": "n3aoXZWfz3be",
    "outputId": "2428f45e-8f68-43d6-a721-064b1d0e38b0"
   },
   "outputs": [],
   "source": [
    "# Seleccionamos las características de la longitud y anchura del pétalo\n",
    "X = iris['data'][:, [2,3]]  # Petal Length, Petal Width\n",
    "# Buscamos clasificar si la planta es del tipo Iris Virginica (1) o no (0) \n",
    "y = (iris['target'] == 2).astype(np.float64)  # Iris Virginica\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KCsfJ2z1ARK-"
   },
   "source": [
    "Esto nos deja con un problema de clasificación:\n",
    "→ ¿La flor es **Virginica** (1) o **no Virginica** (0) ?\n",
    "\n",
    "### 2. Por qué usamos un Pipeline\n",
    "\n",
    "Antes de entrenar un SVM necesitamos escalar las características porque:\n",
    "\n",
    "* Los SVM se basan en distancias (el margen depende de ellas).\n",
    "* Sin escalado, las features con valores grandes dominarían completamente el cálculo.\n",
    "\n",
    "Además, un **pipeline** garantiza que:\n",
    "\n",
    "* El escalado se entrena SOLO con datos de entrenamiento.\n",
    "* La transformación se aplica correctamente en test y predicción futura.\n",
    "* Se evita \"contaminación\" entre fases.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Creación del pipeline con SVM linea\n",
    "\n",
    "#### **StandardScaler**\n",
    "\n",
    "* Resta la media y divide por la desviación estándar.\n",
    "* Hace que todas las características tengan media 0 y varianza 1.\n",
    "* Es imprescindible para que el margen del SVM sea correcto.\n",
    "\n",
    "#### **LinearSVC**\n",
    "\n",
    "Es la implementación eficiente del SVM lineal basada en optimización convexa.\n",
    "\n",
    "Los hiperparámetros importantes:\n",
    "\n",
    "* **C=1**\n",
    "\n",
    "  * Controla cuánta penalización reciben los errores.\n",
    "  * C grande = margen rígido, riesgo de sobreajuste.\n",
    "  * C pequeño = margen flexible, mayor generalización.\n",
    "\n",
    "* **loss='hinge'**\n",
    "\n",
    "  * Usa la función de pérdida típica del SVM.\n",
    "  * Alternativa: `'squared_hinge'` (más suave, pero puede castigar más fuerte los errores).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 376,
     "status": "ok",
     "timestamp": 1678287902514,
     "user": {
      "displayName": "Ernesto Frías Nores",
      "userId": "04050836756016798788"
     },
     "user_tz": -60
    },
    "id": "V_z8rgYVz3bf"
   },
   "outputs": [],
   "source": [
    "svm_clf = Pipeline([\n",
    "    ('Scaler', StandardScaler()),\n",
    "    ('Linear_svc', LinearSVC(C=1, loss='hinge'))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 4. Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 128
    },
    "executionInfo": {
     "elapsed": 335,
     "status": "ok",
     "timestamp": 1678287911776,
     "user": {
      "displayName": "Ernesto Frías Nores",
      "userId": "04050836756016798788"
     },
     "user_tz": -60
    },
    "id": "f4J4t2llz3bg",
    "outputId": "05e089e7-feed-4348-b3bd-79154e8da907"
   },
   "outputs": [],
   "source": [
    "svm_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ¿Qué ocurre internamente?\n",
    "\n",
    "1. El pipeline calcula media y desviación de cada característica (solo sobre X).\n",
    "2. Reescala los datos.\n",
    "3. `LinearSVC` busca el hiperplano con **máximo margen** que separe las clases.\n",
    "4. Identifica los **vectores de soporte**, las muestras que definen la frontera.\n",
    "\n",
    "En este punto el modelo ya está entrenado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SoVFeOX8CXpL"
   },
   "source": [
    "### 5. Predicción con el modelo entrenado\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 361,
     "status": "ok",
     "timestamp": 1678287914650,
     "user": {
      "displayName": "Ernesto Frías Nores",
      "userId": "04050836756016798788"
     },
     "user_tz": -60
    },
    "id": "tqLERIw4z3bg",
    "outputId": "f8fffbce-7f13-4234-8306-560c95bb133a"
   },
   "outputs": [],
   "source": [
    "svm_clf.predict([[5.5, 1.7], [2.0, 1.8]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explicación paso a paso:\n",
    "\n",
    "El pipeline `svm_clf` hace:\n",
    "\n",
    "1. Toma las muestras nuevas.\n",
    "2. Aplica el *mismo escalado* aprendido durante entrenamiento.\n",
    "3. Pasa los datos escalados al SVM.\n",
    "4. El SVM calcula en qué lado del hiperplano quedan.\n",
    "5. Devuelve 0 o 1:\n",
    "\n",
    "* **1** → El modelo cree que es *Virginica*.\n",
    "* **0** → El modelo cree que no lo es."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W6_dX8txz3bh"
   },
   "source": [
    "### Importante: Probabilidades\n",
    "\n",
    "A diferencia de la regresión logística, los SVM **no generan probabilidades** de forma nativa.\n",
    "Para obtener probabilidades calibradas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "calibrated_svm = CalibratedClassifierCV(svm_clf, method='sigmoid')  \n",
    "calibrated_svm.fit(X, y)\n",
    "calibrated_svm.predict_proba([[5.5, 1.7]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este procedimiento aplica *Platt scaling*, que ajusta una sigmoide al margen generado por el SVM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s4Ds_223z3bh"
   },
   "source": [
    "## 2. Clasificación no Lineal con SVM\n",
    "\n",
    "En muchos conjuntos de datos reales, las clases **no pueden separarse mediante un hiperplano lineal**, incluso aunque apliquemos margen suave. Un enfoque clásico consiste en **añadir características no lineales**, por ejemplo, términos polinómicos. Al transformar el espacio (aunque no cambiemos los datos visualmente), las clases pueden volverse separables linealmente.\n",
    "\n",
    "\n",
    "![SVM](./img/img.png)\n",
    "\n",
    "Esta idea se aprecia en la siguiente figura: un conjunto no lineal en 1D se vuelve linealmente separable al añadir una nueva característica $ x_2 = x_1^2 $.\n",
    "\n",
    "<div style=\"text-align:center;\"><img style=\"width:66%;\" src=\"./img/nonlinear_to_linear.png\" /></div>\n",
    "\n",
    "### 2.1. Generación del dataset Moons\n",
    "\n",
    "Veamos a continuación como implementar esta idea usando scikit-learn con el dataset moons. Este dataset está formado por conjuntos de puntos (cuyo número es configurable, en este caso, será de 100) dispuestos en el plano formando dos lunas opuestas::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "executionInfo": {
     "elapsed": 2207,
     "status": "ok",
     "timestamp": 1678287921844,
     "user": {
      "displayName": "Ernesto Frías Nores",
      "userId": "04050836756016798788"
     },
     "user_tz": -60
    },
    "id": "7zxY6F_Jz3bh",
    "outputId": "012cbbdf-2a23-4593-98ed-1b5eedf70711"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "X, y = make_moons(n_samples=100, noise=0.15)\n",
    "import seaborn as sns\n",
    "Cx = X[:, 0]\n",
    "Cy = X[:, 1]\n",
    "\n",
    "sns.scatterplot(x=X[y == 0, 0],y=X[y == 0, 1]);\n",
    "sns.scatterplot(x=X[y == 1, 0],y=X[y == 1, 1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **make_moons** genera dos regiones con forma de media luna.\n",
    "* `noise=0.15` añade ruido para que el problema sea más realista (¡sin ruido sería demasiado fácil!).\n",
    "* Este tipo de dataset **no puede separarse con un SVM lineal**, aunque se ajuste C → ∞.\n",
    "\n",
    "Esto motiva técnicas no lineales.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "syBKVsYvgk9b"
   },
   "source": [
    "### 2.2. SVM con características polinómicas (sin kernels)\n",
    "\n",
    "Antes de llegar al *kernel trick*, implementamos la idea “manual”:\n",
    "\n",
    "> Añadir nuevas características polinómicas → entrenar un SVM lineal en ese espacio ampliado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 313,
     "status": "ok",
     "timestamp": 1678287950018,
     "user": {
      "displayName": "Ernesto Frías Nores",
      "userId": "04050836756016798788"
     },
     "user_tz": -60
    },
    "id": "oTnsVhz2z3bi"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "polynomial_svm_clf = Pipeline([\n",
    "    (\"poly_features\", PolynomialFeatures(degree=3)),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"svm_clf\", LinearSVC(C=10, loss=\"hinge\", max_iter=10000))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1) PolynomialFeatures(degree=3)**\n",
    "\n",
    "Genera automáticamente todas las combinaciones polinómicas hasta grado 3:\n",
    "\n",
    "* $ x_1 $\n",
    "* $ x_2 $\n",
    "* $ x_1^2, x_1 x_2, x_2^2 $\n",
    "* $ x_1^3, x_1^2 x_2, x_1 x_2^2, x_2^3 $\n",
    "* … y el término de sesgo $1$\n",
    "\n",
    "Esto transforma un dataset 2D → 10D.\n",
    "\n",
    "El SVM lineal ahora intenta separar datos en un espacio de 10 dimensiones, donde sí pueden ser linealmente separables.\n",
    "\n",
    "#### **2) StandardScaler**\n",
    "\n",
    "Es fundamental porque:\n",
    "\n",
    "→ Sin escalado, las variables polinómicas crecen muy rápido y arruinan el margen.\n",
    "→ SVM necesita siempre trabajar con features escaladas.\n",
    "\n",
    "#### **3) LinearSVC con C=10**\n",
    "\n",
    "* C más grande = menos tolerancia al error → frontera más compleja.\n",
    "* Con datos ruidosos puede sobreajustar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218
    },
    "executionInfo": {
     "elapsed": 335,
     "status": "ok",
     "timestamp": 1678287954659,
     "user": {
      "displayName": "Ernesto Frías Nores",
      "userId": "04050836756016798788"
     },
     "user_tz": -60
    },
    "id": "lZnS3_95z3bi",
    "outputId": "e415c78d-f85b-46ac-9b90-1b02de05598b"
   },
   "outputs": [],
   "source": [
    "polynomial_svm_clf.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 353,
     "status": "ok",
     "timestamp": 1678287958042,
     "user": {
      "displayName": "Ernesto Frías Nores",
      "userId": "04050836756016798788"
     },
     "user_tz": -60
    },
    "id": "dhX3hDKHz3bj",
    "outputId": "a655d792-3234-456b-cdab-35c7a8afc8d8"
   },
   "outputs": [],
   "source": [
    "polynomial_svm_clf.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Visualización de la frontera de decisión\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 309
    },
    "executionInfo": {
     "elapsed": 757,
     "status": "ok",
     "timestamp": 1678287960858,
     "user": {
      "displayName": "Ernesto Frías Nores",
      "userId": "04050836756016798788"
     },
     "user_tz": -60
    },
    "id": "WyH1DsX2z3bj",
    "outputId": "d87ddf18-bb8b-4412-f6d5-ebb8c580c05e",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_dataset(X, y, axes):\n",
    "    plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"bs\") \n",
    "    plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"g^\")\n",
    "    plt.axis(axes)\n",
    "    plt.grid(True)\n",
    "    plt.xlabel(r\"$x_1$\", fontsize=16)\n",
    "    plt.ylabel(r\"$x_2$\", fontsize=16)\n",
    "    plt.title(\"Datos lunares\", fontsize=18)\n",
    "\n",
    "def plot_predictions(clf, axes):\n",
    "    x0s = np.linspace(axes[0], axes[1], 200)\n",
    "    x1s = np.linspace(axes[2], axes[3], 200)\n",
    "    x0, x1 = np.meshgrid(x0s, x1s)\n",
    "\n",
    "    X_grid = np.c_[x0.ravel(), x1.ravel()]\n",
    "\n",
    "    # ⚠ LinearSVC NO siempre implementa decision_function para pipelines → depende de versión\n",
    "    y_pred = clf.predict(X_grid).reshape(x0.shape)\n",
    "\n",
    "    # Contornos\n",
    "    plt.contourf(x0, x1, y_pred, alpha=0.2, cmap=plt.cm.brg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 309
    },
    "executionInfo": {
     "elapsed": 560,
     "status": "ok",
     "timestamp": 1678287963943,
     "user": {
      "displayName": "Ernesto Frías Nores",
      "userId": "04050836756016798788"
     },
     "user_tz": -60
    },
    "id": "X07y1LlG30aR",
    "outputId": "aec60bd5-fceb-4e7a-ff8f-9646ff953a2c"
   },
   "outputs": [],
   "source": [
    "plot_predictions(polynomial_svm_clf, [-1.5, 2.5, -1, 1.5])\n",
    "plot_dataset(X, y, [-1.5, 2.5, -1, 1.5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TVzu6do8z3bk"
   },
   "source": [
    "El siguiente gráfico muestra los límites de decisión del modelo. Debido a que agregamos grados polinómicos, los límites proyectados ahora son no lineales:\n",
    "\n",
    "<div style=\"text-align:center;\"><img style=\"width:50%;\" src=\"./img/polynomial_svms.png\" /></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8eljbOJaz3bk"
   },
   "source": [
    "### 2.1. Kernels Polinómicos\n",
    "\n",
    "En grados polinómicos bajos, agregar características no permite manejar conjuntos de datos complejos, y para grados polinómicos altos terminamos agregando muchas características, lo que da como resultado un modelo muy complejo y lento.\n",
    "\n",
    "Afortunadamente, cuando usamos SVM podemos aplicar una técnica matemática casi milagrosa llamada **truco del kernel**. El truco del kernel hace posible obtener el mismo resultado que si añadiéramos muchas características polinómicas sin añadirlas realmente.\n",
    "\n",
    "En este punto es importante distinguir entre dos variantes del clasificador SVM que ofrece *scikit-learn*: **SVC** y **LinearSVC**, ya que a menudo se confunden.\n",
    "\n",
    "* **SVC** es la implementación general del clasificador SVM.\n",
    "  Permite usar kernels no lineales (por ejemplo, polinómico o RBF) y, por tanto, puede modelar fronteras de decisión muy complejas. Sin embargo, al trabajar internamente con la matriz de kernel, su coste computacional crece de forma cuadrática o cúbica con el número de muestras. Por este motivo es adecuado solo para datasets pequeños o medianos.\n",
    "\n",
    "* **LinearSVC**, en cambio, está diseñado exclusivamente para problemas **lineales**.\n",
    "  No admite kernels y utiliza un algoritmo de optimización diferente, mucho más eficiente, lo que le permite entrenarse en grandes conjuntos de datos. Es la mejor opción cuando creemos que las clases pueden separarse aproximadamente mediante un hiperplano, o cuando necesitamos escalar a miles o millones de ejemplos.\n",
    "\n",
    "Por tanto:\n",
    "\n",
    "* Si queremos **fronteras no lineales**, necesitamos **SVC** con un kernel adecuado.\n",
    "* Si queremos **velocidad** o trabajar con datasets grandes, usamos **LinearSVC**, aun sabiendo que solo aprenderá fronteras lineales.\n",
    "\n",
    "Vamos a probar el uso de kernels en el dataset *moons*:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1678287966536,
     "user": {
      "displayName": "Ernesto Frías Nores",
      "userId": "04050836756016798788"
     },
     "user_tz": -60
    },
    "id": "IxeFhUwnz3bl"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm  import SVC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1678287968132,
     "user": {
      "displayName": "Ernesto Frías Nores",
      "userId": "04050836756016798788"
     },
     "user_tz": -60
    },
    "id": "Jnd9NSiaz3bl",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "X, y = make_moons(n_samples=300, noise=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1678287969784,
     "user": {
      "displayName": "Ernesto Frías Nores",
      "userId": "04050836756016798788"
     },
     "user_tz": -60
    },
    "id": "u73QWCibz3bl"
   },
   "outputs": [],
   "source": [
    "poly_kernel_svm_clf = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svm_clf', SVC(kernel='poly', degree=3, coef0=1, C=5))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El kernel polinómico calcula:\n",
    "\n",
    "$\n",
    "K(x, z) = (x \\cdot z + r)^d\n",
    "$\n",
    "\n",
    "Esto significa:\n",
    "\n",
    "* No creamos explícitamente características como $x_1^2$, $x_1x_2$…\n",
    "* Pero matemáticamente el SVM se está comportando *como si* esas características existieran.\n",
    "* Esto reduce tremendamente el coste computacional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 128
    },
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1678287971053,
     "user": {
      "displayName": "Ernesto Frías Nores",
      "userId": "04050836756016798788"
     },
     "user_tz": -60
    },
    "id": "g-ZVHEn3z3bl",
    "outputId": "9486af14-65bb-4736-c11d-7f7a207f1c6d"
   },
   "outputs": [],
   "source": [
    "poly_kernel_svm_clf.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 309
    },
    "executionInfo": {
     "elapsed": 525,
     "status": "ok",
     "timestamp": 1678287973460,
     "user": {
      "displayName": "Ernesto Frías Nores",
      "userId": "04050836756016798788"
     },
     "user_tz": -60
    },
    "id": "I5bdbIzuz3bm",
    "outputId": "9c7cb750-138e-425b-cd68-4b2ec8795540",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plot_predictions(poly_kernel_svm_clf, [-1.5, 2.5, -1, 1.5])\n",
    "plot_dataset(X, y, [-1.5, 2.5, -1, 1.5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "FvqxpjO1z3bm",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Este modelo entrena un clasificador SVM utilizando un kernel de features de **tercer grado**.\n",
    "El kernel polinómico permite que el clasificador actúe *como si* estuviera trabajando en un espacio ampliado con todas las combinaciones polinómicas posibles hasta dicho grado, pero sin necesidad de generarlas explícitamente gracias al **truco del kernel**.\n",
    "\n",
    "Si nuestro modelo se ajusta en exceso, es posible que deseemos **disminuir el grado** del polinomio, ya que los polinomios de grado alto tienden a producir fronteras de decisión más onduladas y adaptadas al ruido del dataset (*overfitting*).\n",
    "Por el contrario, si se ajusta de manera insuficiente (subrepresenta la forma de los datos), puede ser una buena idea **aumentar el grado**, lo cual permite que la frontera sea más flexible y capture patrones más complejos.\n",
    "\n",
    "El parámetro `coef0` (a veces llamado *r* en la literatura) controla en qué porcentaje el modelo está influenciado por los **términos de alto grado** frente a los **términos de bajo grado**.\n",
    "En términos prácticos:\n",
    "\n",
    "* Un `coef0` pequeño hace que predominen los términos de menor grado → frontera más suave.\n",
    "* Un `coef0` grande acentúa los términos de alto grado → frontera más compleja y con mayor curvatura.\n",
    "\n",
    "Este parámetro suele tener un impacto importante en datasets como *moons*, donde la curvatura de la frontera es relevante para separar las dos medias lunas.\n",
    "\n",
    "La siguiente figura muestra el modelo previamente entrenado de **grado 3** (a la izquierda), frente a un modelo más complejo con kernel polinómico de **grado 10**.\n",
    "La comparación ilustra cómo, al aumentar el grado, la frontera de decisión se vuelve mucho más intricada —capaz de ajustarse a detalles finos, pero también más propensa al sobreajuste—, mientras que grados más bajos tienden a generar fronteras más estables y generalizables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "ZStHg5LPz3bm"
   },
   "source": [
    "<div style=\"text-align:center;\"><img style=\"width:66%;\" src=\"./img/kernel_trick.png\" /></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Tabla comparativa: LinearSVC vs SVC**\n",
    "\n",
    "| Característica            | **LinearSVC**               | **SVC**                                                        |\n",
    "| ------------------------- | --------------------------- | -------------------------------------------------------------- |\n",
    "| Tipo de modelo            | Clasificador **lineal**     | Clasificador **lineal o no lineal** (según kernel)             |\n",
    "| Soporte de kernels        | ❌ No admite kernels         | ✔️ Admite kernels (`'poly'`, `'rbf'`, `'sigmoid'`, `'linear'`) |\n",
    "| Complejidad computacional | **O(n)** → muy rápido       | **O(n²)–O(n³)** → muy lento                                    |\n",
    "| Dataset recomendado       | Grande o muy grande         | Pequeño o mediano                                              |\n",
    "| Velocidad                 | ⭐⭐⭐⭐⭐                       | ⭐⭐                                                             |\n",
    "| Memoria necesaria         | Baja                        | Alta (matriz kernel n×n)                                       |\n",
    "| Riesgo de overfitting     | Bajo–medio                  | Medio–alto (según kernel)                                      |\n",
    "| Flexibilidad del modelo   | Baja                        | Muy alta (fronteras no lineales)                               |\n",
    "| Interpretación            | Más fácil (frontera lineal) | Puede ser compleja (según kernel)                              |\n",
    "| Similar en regresión      | **LinearSVR**               | **SVR**                                                        |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DGUHsmsez3bn"
   },
   "source": [
    "## 3. Regresión SVM\n",
    "\n",
    "Las SVM también admiten la regresión lineal y no lineal, pero para pasar de la clasificación a la regresión tenemos que invertir el objetivo. En lugar de intentar encajar la calle más grande posible entre dos clases mientras se limitan las infracciones de margen, la regresión SVM intenta encajar **tantas instancias como sea posible** en la calle mientras limita las infracciones de margen. El ancho de la calle está controlado por el hiperparámetro $ \\epsilon $. A continuación se muestra un ejemplo:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "sOxXqoi8z3bn",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div style=\"text-align:center;\"><img style=\"width:66%;\" src=\"./img/SVM_regression.png\" /></div>\n",
    "\n",
    "Agregar más instancias al margen no afecta las predicciones del modelo; por lo tanto, se dice que el modelo es $ \\epsilon $-insensible.\n",
    "\n",
    "Vamos a implementar un regresor lineal `SVR` de `sklearn` (después de escalar y centrar los datos):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Dataset artificial de regresión lineal\n",
    "X = np.linspace(-5, 5, 200).reshape(-1, 1)\n",
    "y = 3 * X.ravel() + 2 + np.random.randn(200) * 2   # recta + ruido\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "plt.scatter(X, y, color=\"blue\", alpha=0.5, label=\"Datos reales\")\n",
    "plt.title(\"Datos artificiales para regresión\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 310,
     "status": "ok",
     "timestamp": 1678287975947,
     "user": {
      "displayName": "Ernesto Frías Nores",
      "userId": "04050836756016798788"
     },
     "user_tz": -60
    },
    "id": "MjPbbozcz3bn",
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "svm_reg = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"svr\", LinearSVR(epsilon=1.5, C=1.0, random_state=42))\n",
    "])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1678287979291,
     "user": {
      "displayName": "Ernesto Frías Nores",
      "userId": "04050836756016798788"
     },
     "user_tz": -60
    },
    "id": "BWqp54OXz3bo",
    "outputId": "0d0d6b2a-ee1e-475f-be6c-e5ea18a07f18",
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "svm_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svm_reg.predict(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "\n",
    "# Datos originales\n",
    "plt.scatter(X, y, color=\"blue\", alpha=0.5, label=\"Datos reales\")\n",
    "\n",
    "# Línea aprendida por LinearSVR\n",
    "plt.plot(X, y_pred, color=\"red\", linewidth=2, label=\"Predicción SVM (LinearSVR)\")\n",
    "\n",
    "plt.title(\"Regresión SVM lineal sobre datos artificiales\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "ijYETwoyz3bo"
   },
   "source": [
    "### 3.1 **Regresor lineal SVR**\n",
    "\n",
    "Para abordar las tareas de regresión lineal, podemos usar un modelo SVM con kernel polinomial.\n",
    "\n",
    "Creamos un conjunto de datos sintético donde la salida sigue una relación cuadrática con la entrada. Este dataset, con algo de ruido añadido, es ideal para mostrar cómo un SVR con kernel polinómico puede ajustarse a patrones no lineales.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1) Dataset artificial cuadrático + ruido\n",
    "\n",
    "np.random.seed(42)\n",
    "X = np.linspace(-3, 3, 200).reshape(-1, 1)\n",
    "y = 0.5 * X.ravel()**2 + 1 + np.random.randn(200) * 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Scatter plot de los datos (ANTES de entrenar el modelo)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(X, y, color=\"blue\", alpha=0.5, label=\"Datos reales\")\n",
    "plt.title(\"Datos artificiales para regresión (cuadrático + ruido)\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Entrenamiento de un modelo SVR con kernel polinómico\n",
    "\n",
    "Una vez generado el dataset artificial, definimos un modelo de regresión SVM capaz de capturar relaciones no lineales entre las variables. Para ello utilizamos un **SVR con kernel polinómico de grado 2**, que permite aprender funciones con forma parabólica. Incluimos también un `StandardScaler` dentro de un `Pipeline` para garantizar que todas las características estén correctamente escaladas antes del entrenamiento, algo esencial para el buen rendimiento de los SVM.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1678287983543,
     "user": {
      "displayName": "Ernesto Frías Nores",
      "userId": "04050836756016798788"
     },
     "user_tz": -60
    },
    "id": "bC_VfnTCz3bo",
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "# 3) Definir el modelo SVR con kernel polinómico (degree=2)\n",
    "\n",
    "svm_poly_reg = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"svr\", SVR(kernel='poly', degree=2, C=100, epsilon=0.1, gamma='auto'))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el modelo SVR con kernel polinómico utilizamos tres hiperparámetros clave que controlan el comportamiento del ajuste:\n",
    "\n",
    "* **C = 100**\n",
    "  Determina cuánto penaliza el modelo los errores fuera del margen $ \\epsilon $.\n",
    "  *Cuanto mayor es C*, más estrictamente intenta el SVR aproximarse a los datos, permitiendo una curva más ajustada pero con mayor riesgo de sobreajuste.\n",
    "\n",
    "* **epsilon = 0.1**\n",
    "  Define el tamaño del “tubo” $ \\epsilon $-insensible alrededor de la función predicha.\n",
    "  Los errores que caen dentro de este margen **no se penalizan**.\n",
    "  Un valor pequeño $0.1$ hace que el modelo sea más sensible y trate de ajustarse con mayor precisión a los puntos.\n",
    "\n",
    "* **gamma = 'auto'**\n",
    "  Es un parámetro que afecta a la influencia de cada punto de entrenamiento en el kernel.\n",
    "  Con `'auto'`, gamma se calcula como:\n",
    "\n",
    "    $\n",
    "    \\gamma = \\frac{1}{n_{\\text{features}}}\n",
    "    $\n",
    "\n",
    "  En este caso, dado que solo hay 1 característica, gamma = 1.\n",
    "  Esto controla la “anchura” de las curvas generadas por el kernel polinómico.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el modelo SVR con kernel polinómico utilizamos tres hiperparámetros clave que controlan el comportamiento del ajuste:\n",
    "\n",
    "* **C = 100**\n",
    "  Determina cuánto penaliza el modelo los errores fuera del margen $ \\epsilon $.\n",
    "  *Cuanto mayor es C*, más estrictamente intenta el SVR aproximarse a los datos, permitiendo una curva más ajustada pero con mayor riesgo de sobreajuste.\n",
    "\n",
    "* **epsilon = 0.1**\n",
    "  Define el tamaño del “tubo” $ \\epsilon $-insensible alrededor de la función predicha.\n",
    "  Los errores que caen dentro de este margen **no se penalizan**.\n",
    "  Un valor pequeño $0.1$ hace que el modelo sea más sensible y trate de ajustarse con mayor precisión a los puntos.\n",
    "\n",
    "* **gamma = 'auto'**\n",
    "  Es un parámetro que afecta a la influencia de cada punto de entrenamiento en el kernel.\n",
    "  Con `'auto'`, gamma se calcula como:\n",
    "\n",
    "    $\n",
    "    \\gamma = \\frac{1}{n_{\\text{features}}}\n",
    "    $\n",
    "\n",
    "  En este caso, dado que solo hay 1 característica, gamma = 1.\n",
    "  Esto controla la “anchura” de las curvas generadas por el kernel polinómico.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "executionInfo": {
     "elapsed": 315,
     "status": "ok",
     "timestamp": 1678287984280,
     "user": {
      "displayName": "Ernesto Frías Nores",
      "userId": "04050836756016798788"
     },
     "user_tz": -60
    },
    "id": "gTxnQgUIz3bp",
    "outputId": "71c6258f-79f5-4028-ca82-ece1b382eb27",
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Entrenamiento del modelo\n",
    "svm_poly_reg.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Predicción sobre los mismos puntos X\n",
    "y_pred = svm_poly_reg.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Dibujar de nuevo el scatter + la línea de regresión aprendida\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(X, y, color=\"blue\", alpha=0.5, label=\"Datos reales\")\n",
    "plt.plot(X, y_pred, color=\"red\", linewidth=2, label=\"SVR polinómico grado 2\")\n",
    "plt.title(\"Regresión SVM con kernel polinómico (degree=2)\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **Conclusión: diferencia entre LinearSVR y SVR**\n",
    "\n",
    "`LinearSVR` y `SVR` implementan ambos la regresión con máquinas de soporte vectorial, pero utilizan algoritmos muy distintos y están pensados para problemas diferentes.\n",
    "\n",
    "* **LinearSVR** usa un método de optimización específico para modelos lineales.\n",
    "  Es extremadamente rápido y escala bien incluso con decenas o cientos de miles de muestras. Solo puede aprender **relaciones lineales** entre las variables.\n",
    "\n",
    "* **SVR**, en cambio, es la implementación completa del algoritmo basado en kernels.\n",
    "  Puede aprender **relaciones no lineales** gracias al kernel polinómico, RBF u otros, pero su coste computacional crece de manera cuadrática o cúbica con el número de muestras, por lo que solo es práctico para datasets pequeños o medianos.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**Tabla comparativa: LinearSVR vs SVR**\n",
    "\n",
    "| Característica            | **LinearSVR**         | **SVR**                                                       |\n",
    "| ------------------------- | --------------------- | ------------------------------------------------------------- |\n",
    "| Tipo de modelo            | Regressión **lineal** | Regresión **lineal o no lineal** (según kernel)               |\n",
    "| Soporte de kernels        | ❌ No admite kernels   | ✔️ Admite kernels: `'poly'`, `'rbf'`, `'sigmoid'`, `'linear'` |\n",
    "| Complejidad computacional | **O(n)** → muy rápido | **O(n²)–O(n³)** → muy lento                                   |\n",
    "| Dataset recomendado       | Grande o muy grande   | Pequeño o mediano                                             |\n",
    "| Velocidad                 | ⭐⭐⭐⭐⭐                 | ⭐⭐                                                            |\n",
    "| Memoria necesaria         | Baja                  | Alta (matriz kernel n×n)                                      |\n",
    "| Riesgo de overfitting     | Bajo–medio            | Medio–alto (según kernel y parámetros)                        |\n",
    "| Flexibilidad del modelo   | Baja                  | Muy alta                                                      |\n",
    "| Similar en clasificación  | **LinearSVC**         | **SVC**                                                       |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "F5nnrbJCz3bp"
   },
   "source": [
    "\n",
    "\n",
    "## 4. EXTRA: Similarity Features\n",
    "\n",
    "Al margen de los kernels polinómicos, existe otra técnica habitual para abordar problemas no lineales llamada **“similarity features”**. Esta técnica consiste en agregar características adicionales calculadas mediante una **función de similitud**, la cual mide cuánto se parece cada instancia a uno o varios **puntos de referencia** específicos.\n",
    "\n",
    "Por ejemplo, si tomamos el dataset visto anteriormente y agregamos dos puntos de referencia en $ x_1 = -2 $ y $ x_1 = 1 $, como se muestra en el gráfico de la izquierda:\n",
    "\n",
    "<div style=\"text-align:center;\"><img style=\"width:66%;\" src=\"./img/similarity_measures.png\" /></div>\n",
    "\n",
    "Definimos la función de similitud como la **Función de Base Radial Gaussiana (RBF)** con $ \\gamma = 0.3 $.\n",
    "La función $ \\phi $ es una función en forma de campana que toma valores:\n",
    "\n",
    "* cercanos a **1** cuando la instancia $ x $ está cerca del punto de referencia $ l $,\n",
    "* cercanos a **0** cuando $ x $ está lejos del punto de referencia.\n",
    "\n",
    "Matemáticamente:\n",
    "\n",
    "$\n",
    "\\phi(x, , l) = \\exp(-\\gamma \\lVert x - l \\rVert^2)\n",
    "$\n",
    "\n",
    "Después de definir la función de similitud, las nuevas características que añadimos al dataset son precisamente las **distancias suavizadas** (según la función RBF) entre cada instancia y los puntos de referencia elegidos.\n",
    "\n",
    "Como se aprecia en el gráfico de la derecha, al representar las instancias en este nuevo espacio construido a partir de las “distancias a puntos de referencia”, los datos se vuelven **linealmente separables**. Es decir, al transformar el espacio mediante funciones de similitud, una frontera lineal en el nuevo espacio puede corresponder a una frontera no lineal en el espacio original.\n",
    "\n",
    "Pero surge una pregunta fundamental: **¿cómo seleccionamos los puntos de referencia?**\n",
    "\n",
    "El enfoque más simple es usar **todos los puntos de entrenamiento como puntos de referencia**. Esto transforma el espacio original de dimensión $ n $ a un nuevo espacio de dimensión $ m $, donde $ m $ es el número total de instancias del dataset.\n",
    "\n",
    "Este aumento masivo de dimensiones:\n",
    "\n",
    "* proporciona una gran capacidad para encontrar un separador lineal en el espacio transformado,\n",
    "* pero tiene el inconveniente de que el coste computacional crece en la misma proporción,\n",
    "* lo que puede generar problemas de rendimiento, especialmente si el número de instancias es grande.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "yqHbLRBmz3bp"
   },
   "source": [
    "### 4.1. Gaussian RBF Kernel\n",
    "\n",
    "Al igual que el método de características polinómicas, el método de características de similitud puede ser útil en muchos algoritmos de ML. El problema es que con conjuntos de datos muy grandes, terminaremos con un espacio de características muy grande, pero una vez más tenemos el truco de Kernel para que parezca como si añadimos características adicionales.\n",
    "\n",
    "Vamos a verlo con `sklearn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 760,
     "status": "ok",
     "timestamp": 1678287987560,
     "user": {
      "displayName": "Ernesto Frías Nores",
      "userId": "04050836756016798788"
     },
     "user_tz": -60
    },
    "id": "kK-TA4P8z3bq",
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "X, y = make_moons(n_samples=100, noise=0.15)\n",
    "import seaborn as sns\n",
    "Cx = X[:, 0]\n",
    "Cy = X[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1678287987906,
     "user": {
      "displayName": "Ernesto Frías Nores",
      "userId": "04050836756016798788"
     },
     "user_tz": -60
    },
    "id": "R5e7gCYGz3bq",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "rbf_kernel_svm_clf = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"svm_clf\", SVC(kernel=\"rbf\", gamma=0.1, C=1000))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 128
    },
    "executionInfo": {
     "elapsed": 454,
     "status": "ok",
     "timestamp": 1678287989481,
     "user": {
      "displayName": "Ernesto Frías Nores",
      "userId": "04050836756016798788"
     },
     "user_tz": -60
    },
    "id": "puiHAjkzz3bq",
    "outputId": "1a7186d9-f4bc-4d64-a590-9372926c4730",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "rbf_kernel_svm_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions(rbf_kernel_svm_clf, [-1.5, 2.5, -1, 1.5])\n",
    "plot_dataset(X, y, [-1.5, 2.5, -1, 1.5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "FwXRMxYHz3bq",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "A continuación tenemos el espacio de predicciones con las instancias del conjunto de entrenamiento (abajo a la izquierda está el modelo entrenado anteriormente). Los otros gráficos se corresponden a diferentes configuraciones de hiperparámetros:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center;\"><img style=\"width:66%;\" src=\"./img/training_rbfs.png\" /></div>\n",
    "\n",
    "El aumento de $\\gamma$ hace que la curva en forma de campana sea más estrecha, el límite de decisión termina siendo más irregular, moviéndose alrededor de instancias individuales. Entonces $\\gamma$ actúa como un hiperparámetro de regularización.\n",
    "- El aumento de gamma aumenta la sensibilidad del modelo (puede dar lugar a un sobreajuste).\n",
    "- la disminución de gamma aumenta el sesgo del modelo (puede conducir a un ajuste insuficiente)\n",
    "\n",
    "Siempre deberíamos probar primero el kernel lineal, y si el conjunto de entrenamiento no es demasiado grande, entonces podríamos probar el kernel RBF gaussiano."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "Wh3kztkrz3bq"
   },
   "source": [
    "## Ejercicios:\n",
    "\n",
    "Responde a las siguientes preguntas cortas:\n",
    "\n",
    "<div style=\"background-color:green;color:white\">\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "1. ¿Qué es un vector de soporte?\n",
    "2. ¿Es necesario escalar las features cuando usamos SVM? ¿Por qué?\n",
    "3. Supongamos que has entrenado un clasificador SVM con Kernel Polinómicos, pero parece que no se ajusta bien a los datos de entrenamiento. ¿Qué harías?\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oOCoGkXhz3br"
   },
   "source": [
    "### **4. Clasificación lineal con LinearSVC y SVC**\n",
    "\n",
    "En este ejercicio generaremos un dataset sintético linealmente separable, entrenaremos dos clasificadores SVM lineales (`LinearSVC` y `SVC(kernel='linear')`) y compararemos sus fronteras de decisión.\n",
    "Debes **completar el código y los huecos marcados** además de responder brevemente a algunas cuestiones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### **4.1. Generación del dataset**\n",
    "\n",
    "Generamos dos nubes de puntos que representarán dos clases separables linealmente.\n",
    "\n",
    "<div style=\"background-color:green;color:white\">\n",
    "\n",
    "TAREA: completa las líneas marcadas con *#COMPLETAR*\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 305,
     "status": "ok",
     "timestamp": 1678288134435,
     "user": {
      "displayName": "Ernesto Frías Nores",
      "userId": "04050836756016798788"
     },
     "user_tz": -60
    },
    "id": "G9flW7iuz3br",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Clase 1 (roja)\n",
    "d1 = np.concatenate((\n",
    "    np.random.normal(loc=7, scale=1, size=(500, 2)),\n",
    "    np.ones((500, 1))\n",
    "), axis=1)\n",
    "\n",
    "# Clase 0 (azul)\n",
    "d2 = np.concatenate((\n",
    "    np.random.normal(loc=3, scale=1, size=(500, 2)),\n",
    "    np.zeros((500, 1))\n",
    "), axis=1)\n",
    "\n",
    "# COMBINAR LOS DOS GRUPOS\n",
    "d = np.concatenate((d1, d2), axis=0)\n",
    "\n",
    "# MEZCLAR (shuffle)\n",
    "## COMPLETAR ##\n",
    "np.random.shuffle(_____)\n",
    "\n",
    "# REPRESENTACIÓN\n",
    "plt.scatter(d1[:,0], d1[:,1], s=10, c='red')\n",
    "plt.scatter(d2[:,0], d2[:,1], s=10, c='blue')\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "plt.title(\"Datos sintéticos 2D\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v2AFYgdBz3br"
   },
   "source": [
    "---\n",
    "\n",
    "<div style=\"background-color:green;color:white\">\n",
    "\n",
    "<br>\n",
    "\n",
    "#### **4.2. Separación entrenamiento / prueba**\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 308,
     "status": "ok",
     "timestamp": 1678288137430,
     "user": {
      "displayName": "Ernesto Frías Nores",
      "userId": "04050836756016798788"
     },
     "user_tz": -60
    },
    "id": "CZd4IIDVz3br",
    "outputId": "2de3c84c-9579-4cb2-b122-2ae7d6ae3598",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = d[:, :2]   # características\n",
    "y = d[:, 2:]   # etiquetas\n",
    "\n",
    "# COMPLETAR: hacer un split del 20% para test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    _______, _______, test_size=______\n",
    ")\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<div style=\"background-color:green;color:white\">\n",
    "\n",
    "<br>\n",
    "\n",
    "#### **4.3. Entrena un modelo LinearSVC**\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 409,
     "status": "ok",
     "timestamp": 1678288142053,
     "user": {
      "displayName": "Ernesto Frías Nores",
      "userId": "04050836756016798788"
     },
     "user_tz": -60
    },
    "id": "fjS9rYJLID84",
    "outputId": "1920853d-f315-43e6-a3ad-bd443ebe04c0"
   },
   "outputs": [],
   "source": [
    "# Crear el modelo Linear SVC\n",
    "## COMPLETAR ##\n",
    "model = _______\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENTRENAR EL MODELO\n",
    "## COMPLETAR ##\n",
    "model.fit(_______, _______.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUAR\n",
    "acc_linear = model.score(X_test, y_test)\n",
    "acc_linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<div style=\"background-color:green;color:white\">\n",
    "\n",
    "<br>\n",
    "\n",
    "#### **4.4. Visualización de la frontera de decisión (LinearSVC)**\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "executionInfo": {
     "elapsed": 389,
     "status": "ok",
     "timestamp": 1678288319717,
     "user": {
      "displayName": "Ernesto Frías Nores",
      "userId": "04050836756016798788"
     },
     "user_tz": -60
    },
    "id": "8hLnYRlPz3br",
    "outputId": "a2026f85-5b80-4fdc-c9a0-2b4c3283009e",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(d1[:,0], d1[:,1], c='red', s=7)\n",
    "plt.scatter(d2[:,0], d2[:,1], c='blue', s=7)\n",
    "\n",
    "# Parámetros del hiperplano\n",
    "w = model.coef_[0]            # vector normal del hiperplano\n",
    "a = -w[0] / w[1]              # pendiente\n",
    "\n",
    "xx = np.linspace(0, 10)\n",
    "yy = a * xx - (model.intercept_[0]) / _______     # COMPLETAR\n",
    "\n",
    "plt.plot(xx, yy, \"k-\", linewidth=2)\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "plt.title(\"Frontera LinearSVC\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RWSqcBuWz3bs"
   },
   "source": [
    "---\n",
    "\n",
    "<div style=\"background-color:green;color:white\">\n",
    "\n",
    "<br>\n",
    "\n",
    "#### **Entrenamiento con SVC(kernel='linear')**\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 330,
     "status": "ok",
     "timestamp": 1678288323682,
     "user": {
      "displayName": "Ernesto Frías Nores",
      "userId": "04050836756016798788"
     },
     "user_tz": -60
    },
    "id": "ZhZCbJ71z3bs",
    "outputId": "91cbb1a6-beb8-4a6a-c424-f2aed5d04b90",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "svc_clf = SVC(kernel=\"linear\", C=______, gamma=\"auto\")   # COMPLETAR\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1678288324735,
     "user": {
      "displayName": "Ernesto Frías Nores",
      "userId": "04050836756016798788"
     },
     "user_tz": -60
    },
    "id": "wDkP4y7Tz3bs",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# ENTRENAR\n",
    "svc_clf.fit(_______, _______.ravel())       # COMPLETAR\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUAR\n",
    "acc_svc = svc_clf.score(X_test, y_test)\n",
    "acc_svc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cElD5Cclz3bs"
   },
   "source": [
    "---\n",
    "\n",
    "<div style=\"background-color:green;color:white\">\n",
    "\n",
    "<br>\n",
    "\n",
    "#### **4.6. Visualización de la frontera de SVC**\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 325,
     "status": "ok",
     "timestamp": 1678288333981,
     "user": {
      "displayName": "Ernesto Frías Nores",
      "userId": "04050836756016798788"
     },
     "user_tz": -60
    },
    "id": "nxqtGXTFz3bt",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "h = 0.02\n",
    "x_min, x_max = d[:,0].min() - 1, d[:,0].max() + 1\n",
    "y_min, y_max = d[:,1].min() - 1, d[:,1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "\n",
    "Z = svc_clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(d1[:,0], d1[:,1], c='red', s=7)\n",
    "plt.scatter(d2[:,0], d2[:,1], c='blue', s=7)\n",
    "\n",
    "# DIBUJAR LA FRONTERA\n",
    "## COMPLETAR ##\n",
    "plt.contour(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "plt.title(\"Frontera SVC(kernel='linear')\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<div style=\"background-color:green;color:white\">\n",
    "\n",
    "<br>\n",
    "\n",
    "#### ***4.7. Comparación entre LinearSVC y SVC***\n",
    "\n",
    "1. ¿Las fronteras son idénticas?\n",
    "2. ¿En qué se diferencian visualmente?\n",
    "3. ¿Por qué ocurre esto? (pistas: función de pérdida, valor de C)\n",
    "4. ¿Podrían ser exactamente iguales ajustando parámetros?\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<div style=\"background-color:green;color:white\">\n",
    "\n",
    "<br>\n",
    "\n",
    "#### ***4.8. Ajuste para obtener modelos equivalentes***\n",
    "\n",
    "Ahora ajustamos **LinearSVC** para que use la función de pérdidas hinge y el mismo C que SVC.\n",
    "\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 323,
     "status": "ok",
     "timestamp": 1678288340774,
     "user": {
      "displayName": "Ernesto Frías Nores",
      "userId": "04050836756016798788"
     },
     "user_tz": -60
    },
    "id": "hhxJE0o1z3bt",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "C = ______     # mismo valor que en SVC\n",
    "\n",
    "model2 = LinearSVC(C=C, loss=\"squared_hinge\")\n",
    "model2.fit(X_train, y_train.ravel())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 331,
     "status": "ok",
     "timestamp": 1678288342901,
     "user": {
      "displayName": "Ernesto Frías Nores",
      "userId": "04050836756016798788"
     },
     "user_tz": -60
    },
    "id": "KebHc28Wz3bt",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "model2.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<div style=\"background-color:green;color:white\">\n",
    "\n",
    "<br>\n",
    "\n",
    "#### ***4.9. Visualización de la nueva frontera LinearSVC***\n",
    "\n",
    "Ahora ajustamos **LinearSVC** para que use la función de pérdidas hinge y el mismo C que SVC.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1678288344190,
     "user": {
      "displayName": "Ernesto Frías Nores",
      "userId": "04050836756016798788"
     },
     "user_tz": -60
    },
    "id": "LIMFqk-xz3bt",
    "outputId": "418cdc24-b5ba-4ca0-d87c-55b171f95648",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(d1[:,0], d1[:,1], c='red', s=7)\n",
    "plt.scatter(d2[:,0], d2[:,1], c='blue', s=7)\n",
    "\n",
    "w = model2.coef_[0]\n",
    "a = -w[0] / w[1]\n",
    "xx = np.linspace(0, 10)\n",
    "yy = a * xx - (model2.intercept_[0]) / w[1]\n",
    "\n",
    "plt.plot(xx, yy, \"k-\", linewidth=2)\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\")\n",
    "plt.title(\"Frontera LinearSVC (configuración ajustada)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<div style=\"background-color:green;color:white\">\n",
    "\n",
    "<br>\n",
    "\n",
    "#### ***4.10. Reflexión final***\n",
    "\n",
    "* ¿Coincide ahora la frontera de LinearSVC con la de SVC?\n",
    "* ¿Sigue habiendo ligeras diferencias? ¿Por qué?\n",
    "  (pista: *distintos algoritmos internos: liblinear vs libsvm*)\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B7sVvb_Dz3bw"
   },
   "source": [
    "### 5. **Ejercicio 5 — Entrena un Regresor SVM sobre el dataset California Housing (SVR)**\n",
    "\n",
    "En este ejercicio trabajaremos con el dataset real **California Housing**, uno de los clásicos para tareas de regresión. Vamos a:\n",
    "\n",
    "1. Cargar el dataset.\n",
    "2. Escalar correctamente sus variables.\n",
    "3. Entrenar un **SVR** (SVM para regresión).\n",
    "4. Evaluar el rendimiento del modelo.\n",
    "5. Completar código y reflexionar sobre los resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **5.1. Carga del dataset**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 339,
     "status": "ok",
     "timestamp": 1678288620649,
     "user": {
      "displayName": "Ernesto Frías Nores",
      "userId": "04050836756016798788"
     },
     "user_tz": -60
    },
    "id": "DEVmNRd_z3bw",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pprint import pprint\n",
    "\n",
    "# Cargar dataset California Housing\n",
    "dataset = datasets.fetch_california_housing()\n",
    "\n",
    "# Descripción del dataset\n",
    "pprint(dataset.DESCR)\n",
    "\n",
    "X = dataset.data\n",
    "y = dataset.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<div style=\"background-color:green;color:white\">\n",
    "\n",
    "<br>\n",
    "\n",
    "* ¿Qué representan X y y?\n",
    "* ¿Qué tipo de problema es este?\n",
    "* ¿Qué dimensiones tiene X (nº muestras × nº características)?\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **5.2. División entrenamiento / validación**\n",
    "\n",
    "<div style=\"background-color:green;color:white\">\n",
    "\n",
    "<br>\n",
    "\n",
    "completar los parámetros de *train_test_split*\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 825,
     "status": "ok",
     "timestamp": 1678288622501,
     "user": {
      "displayName": "Ernesto Frías Nores",
      "userId": "04050836756016798788"
     },
     "user_tz": -60
    },
    "id": "Lt_FG2Jbz3bw",
    "outputId": "140565ab-10ef-446a-ca64-1a3e05abd66f",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = _____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **5.3. Escalado de datos**\n",
    "\n",
    "<div style=\"background-color:green;color:white\">\n",
    "\n",
    "<br>\n",
    "\n",
    "**IMPORTANTE:**\n",
    "SVR **siempre requiere escalado** de características.\n",
    "Sin escalado, su rendimiento es pésimo.\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1678288625190,
     "user": {
      "displayName": "Ernesto Frías Nores",
      "userId": "04050836756016798788"
     },
     "user_tz": -60
    },
    "id": "xFMCAZfMz3bw",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = ________\n",
    "\n",
    "X_train_scaled = ________\n",
    "\n",
    "X_val_scaled = ________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **5.4. Entrena el modelo SVR**\n",
    "\n",
    "<div style=\"background-color:green;color:white\">\n",
    "\n",
    "<br>\n",
    "\n",
    "**TAREA: completar parámetros del modelo para experimentar (kernel, C, epsilon…).**\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1678288626556,
     "user": {
      "displayName": "Ernesto Frías Nores",
      "userId": "04050836756016798788"
     },
     "user_tz": -60
    },
    "id": "fBj77MP9z3bx",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Crear el modelo SVR \n",
    "regressor = ______\n",
    "\n",
    "regressor.fit(______)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kfumw1yqz3bx"
   },
   "source": [
    "#### **5.5. Evalúa el modelo**\n",
    "\n",
    "<div style=\"background-color:green;color:white\">\n",
    "\n",
    "<br>\n",
    "\n",
    "**TAREA: completar la llamada al método score con los datos escalados.**\n",
    "\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1678288627318,
     "user": {
      "displayName": "Ernesto Frías Nores",
      "userId": "04050836756016798788"
     },
     "user_tz": -60
    },
    "id": "NKL5dnwSz3bx",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Evaluar el modelo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`score` en regresión devuelve **R² (coeficiente de determinación)**.\n",
    "Valores típicos:\n",
    "\n",
    "* 1.0 → ajuste perfecto\n",
    "* 0.7–0.8 → buen modelo\n",
    "* < 0 → el modelo es peor que predecir la media"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **5.6. Reflexión guiada**\n",
    "\n",
    "<div style=\"background-color:green;color:white\">\n",
    "\n",
    "<br>\n",
    "\n",
    "**a) Explica brevemente por qué SVR necesita escalado obligatorio.**\n",
    "\n",
    "\n",
    "**b) Prueba diferentes hiperparámetros del modelo:**\n",
    "Los alumnos deben modificar código:\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1678288628949,
     "user": {
      "displayName": "Ernesto Frías Nores",
      "userId": "04050836756016798788"
     },
     "user_tz": -60
    },
    "id": "DqTdmrgQz3bx",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "regressor = SVR(kernel='rbf', C=10, epsilon=0.1, gamma='scale')\n",
    "regressor.fit(X_train_scaled, y_train)\n",
    "regressor.score(X_val_scaled, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:green;color:white\">\n",
    "\n",
    "<br>\n",
    "\n",
    "**TAREA:**\n",
    "\n",
    "* Cambiar C: 1, 10, 100\n",
    "* Cambiar epsilon: 0.1, 0.5\n",
    "* Cambiar kernel: 'linear', 'poly', 'rbf'\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"background-color:green;color:white\">\n",
    "<br>\n",
    "\n",
    "**c) Completa la tabla con los resultados obtenidos.**\n",
    "\n",
    "| Kernel | C | epsilon | R² obtenido |\n",
    "| ------ | - | ------- | ----------- |\n",
    "|        |   |         |             |\n",
    "|        |   |         |             |\n",
    "\n",
    "\n",
    "**d) Razona cuál configuración parece más adecuada para este dataset y por qué.**\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **5.7. EXTRA (Opcional)**\n",
    "\n",
    "Para profundizar:\n",
    "\n",
    "<div style=\"background-color:green;color:white\">\n",
    "\n",
    "<br>\n",
    "\n",
    "**Añadir código para GridSearchCV y seleccionar los mejores hiperparámetros**:\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "Q3qIUatyz3by"
   },
   "source": [
    "### 6. **Ejercicio EXTRA — Clasificación de dígitos MNIST con SVM (One-vs-Rest)**\n",
    "\n",
    "En este ejercicio entrenarás un **clasificador SVM** para identificar los 10 dígitos del conjunto de datos **MNIST**.\n",
    "\n",
    "Un clasificador SVM es binario por naturaleza, por lo que scikit-learn aplica automáticamente la estrategia **One-vs-Rest (OvR)** para resolver problemas multiclase: entrenará un clasificador por cada dígito (0–9) y decidirá la clase final según cuál produzca el valor de decisión más alto.\n",
    "\n",
    "Dado que entrenar SVM sobre MNIST puede ser costoso, utilizaremos un subconjunto reducido para realizar pruebas rápidas y comprender el proceso, y después podrás experimentar con más datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. **Carga del dataset MNIST**\n",
    "\n",
    "Usamos la versión incluida en `sklearn.datasets`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ASanZRh9z3by",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "import numpy as np\n",
    "\n",
    "# Descargar MNIST (70 000 imágenes de 28x28)\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "\n",
    "X = mnist.data\n",
    "y = mnist.target.astype(np.int8)\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:green;color:white\">\n",
    "\n",
    "<br>\n",
    "\n",
    "* ¿Por qué convertimos *y* a *int8*?\n",
    "* ¿Cuál es el tamaño (n_samples, n_features) de MNIST?\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. **Normalización de los datos**\n",
    "\n",
    "<div style=\"background-color:green;color:white\">\n",
    "\n",
    "<br>\n",
    "\n",
    "Los SVM funcionan **muchísimo mejor** cuando los datos están escalados.\n",
    "\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 60528,
     "status": "ok",
     "timestamp": 1678265048331,
     "user": {
      "displayName": "Ernesto Frías Nores",
      "userId": "04050836756016798788"
     },
     "user_tz": -60
    },
    "id": "bEqqMYSoz3by",
    "outputId": "152fd16b-9b43-40e2-bc2f-03352bd06d59",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = ______\n",
    "\n",
    "X_scaled = ________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. **División entrenamiento / validación**\n",
    "\n",
    "<div style=\"background-color:green;color:white\">\n",
    "\n",
    "<br>\n",
    "\n",
    "* Completar ambos *train_test_split*\n",
    "* Explicar por qué usamos un subconjunto (velocidad / coste computacional)\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z0TpUWqYz3by",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Usamos un conjunto más pequeño para pruebas rápidas\n",
    "X_small, _, y_small, _ = train_test_split(\n",
    "    X_scaled, y, test_size=0.90, random_state=42\n",
    ")\n",
    "\n",
    "# AHORA dividimos el subconjunto en train/val\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    _______, _______, test_size=0.2, random_state=42    # COMPLETAR\n",
    ")\n",
    "\n",
    "X_train.shape, X_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.  **Entrenar un clasificador SVM lineal (rápido)**\n",
    "\n",
    "Primero probamos con **LinearSVC** para tener una referencia.\n",
    "\n",
    "<div style=\"background-color:green;color:white\">\n",
    "\n",
    "**completa hiperparámetros y *.fit()*:**\n",
    "\n",
    "<br>\n",
    "\n",
    "* Probar diferentes valores de C: 1, 5, 10\n",
    "* Anotar resultados (pequeña tabla)\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 328,
     "status": "ok",
     "timestamp": 1678265061342,
     "user": {
      "displayName": "Ernesto Frías Nores",
      "userId": "04050836756016798788"
     },
     "user_tz": -60
    },
    "id": "RG2Ztpbmz3bz",
    "outputId": "e8f4a9ed-c73a-44f2-b447-a410915f9098",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "lin_svm = LinearSVC(C=______, loss=\"hinge\")  # COMPLETAR\n",
    "\n",
    "lin_svm.fit(______, ______)                 # COMPLETAR\n",
    "\n",
    "lin_svm.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.  **Entrenar un SVM con kernel RBF (más preciso pero lento)**\n",
    "\n",
    "El kernel RBF suele dar mejor precisión en MNIST, pero es MUCHO más lento.\n",
    "\n",
    "Por eso entrenaremos también una versión reducida.\n",
    "\n",
    "<div style=\"background-color:green;color:white\">\n",
    "\n",
    "completar los parámetros y entrenar\n",
    "\n",
    "Sugerencias para probar (pero debes elegir):**\n",
    "\n",
    "* C = 2, 5\n",
    "* gamma = \"scale\" o 0.01\n",
    "\n",
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zdgGFmQiz3bz",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "rbf_svm = SVC(\n",
    "    kernel=\"rbf\",\n",
    "    C=______,            # COMPLETAR\n",
    "    gamma=______,        # COMPLETAR\n",
    ")\n",
    "\n",
    "rbf_svm.fit(X_train, y_train)\n",
    "\n",
    "rbf_svm.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.  **Comparación y reflexión**\n",
    "\n",
    " **Rellenar esta tabla con tus resultados reales:**\n",
    "\n",
    "| Modelo    | C | gamma | Precisión Validación |\n",
    "| --------- | - | ----- | -------------------- |\n",
    "| LinearSVC |   |       |                      |\n",
    "| SVC RBF   |   |       |                      |\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"background-color:green;color:white\">\n",
    "\n",
    "* ¿Qué modelo obtiene mejor precisión?\n",
    "* ¿Cuál tarda más en entrenarse? ¿Por qué?\n",
    "* ¿Tiene sentido entrenar un SVM RBF con los 70 000 ejemplos completos?\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.  **(Extra) Entrenar con un conjunto mayor**\n",
    "\n",
    "\n",
    "\n",
    "<div style=\"background-color:green;color:white\">\n",
    "\n",
    "completar este bloque si quieres más precisión\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JfwGx1hDz3bz",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Ampliar el subconjunto\n",
    "X_big, _, y_big, _ = train_test_split(\n",
    "    X_scaled, y, test_size=0.75, random_state=42\n",
    ")\n",
    "\n",
    "# ENTRENAR nuevamente (completar)\n",
    "rbf_svm.fit(______, ______)\n",
    "rbf_svm.score(______, ______)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.  **(Extra) Probar con nuevos números distintos modelos**\n",
    "\n",
    "\n",
    "1.  **Abrir Paint:** Abre el programa Microsoft Paint.\n",
    "2.  **Configurar el lienzo:**\n",
    "      * No intentes dibujar en 28x28 píxeles (es demasiado pequeño para el ratón).\n",
    "      * Haz un cuadrado cómodo, por ejemplo **300x300 píxeles**. (Puedes ajustar el tamaño arrastrando las esquinas del lienzo blanco).\n",
    "3.  **Poner el Fondo Negro:**\n",
    "      * Selecciona la herramienta **Relleno de color** (el ícono del bote de pintura).\n",
    "      * Selecciona el color **Negro** en la paleta.\n",
    "      * Haz clic en el lienzo blanco. Ahora todo debería ser negro.\n",
    "4.  **Configurar el Pincel (Importante):**\n",
    "      * Selecciona la herramienta **Pincel** (recomiendo el primero, el sólido).\n",
    "      * Selecciona el color **Blanco** en la paleta.\n",
    "      * **Grosor:** Cambia el tamaño al **máximo** (la línea más gorda). MNIST usa trazos gruesos; si dibujas fino, el modelo fallará.\n",
    "5.  **Dibujar:**\n",
    "      * Dibuja un número (del 0 al 9) en el centro. Intenta que ocupe una buena parte del lienzo, pero sin tocar los bordes.\n",
    "6.  **Guardar:**\n",
    "      * Ve a `Archivo` \\> `Guardar como` \\> `Imagen PNG`.\n",
    "      * Nómbralo `dibujo.png`.\n",
    "      * Guárdalo en la **misma carpeta** donde tienes tu archivo de Python (.ipynb o .py).\n",
    "\n",
    "-----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Cargar la imagen\n",
    "img = Image.open(\"dibujo.png\").convert(\"L\")\n",
    "\n",
    "# 2. Invertir colores -> Si no lo dibujas con fondo negro\n",
    "# img = Image.eval(img, lambda x: 255 - x)  \n",
    "\n",
    "# 3. Redimensionar a 28x28\n",
    "img = img.resize((28, 28))\n",
    "\n",
    "# 4. Convertir a array y normalizar\n",
    "img_arr = np.array(img) / 255.0\n",
    "\n",
    "# 5. Visualizar para confirmar\n",
    "plt.figure(figsize=(3,3))\n",
    "plt.imshow(img_arr, cmap=\"gray\")\n",
    "plt.title(\"Lo que ve el modelo\")\n",
    "plt.show()\n",
    "\n",
    "# 6. Predecir\n",
    "X_digit = img_arr.reshape(1, -1)\n",
    "# (Asegúrate de escalar si tu modelo usa scaler)\n",
    "X_digit_scaled = scaler.transform(X_digit) \n",
    "\n",
    "# Añade aquí tu modelo entrenado\n",
    "pred = ______.predict(X_digit_scaled)[0]\n",
    "print(\"Predicción:\", pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.  **Conclusión Final del Ejercicio**\n",
    "\n",
    "<div style=\"background-color:green;color:white\">\n",
    "\n",
    "* ¿Cuál hiperparámetro consideras más importante: C o gamma?\n",
    "* ¿Cómo influye el escalado en el rendimiento del SVM?\n",
    "* ¿Qué precisión máxima has logrado?\n",
    "\n",
    "<br>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1of67fY5yK90OX89Q_TTr70mpdfzBqmDq",
     "timestamp": 1678272615922
    },
    {
     "file_id": "1Eq3ltk6DXGsqELg6JjTgLP8p9Q6SS0Tu",
     "timestamp": 1677679837549
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
